{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('???')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv(\"big_mart_sales.csv\")\n",
    "orig_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.Outlet_Size.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.Item_Fat_Content.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original, there 11 features (variables) and 1 output ('Item_Outlet_Sales').\n",
    "# To increase number of features, we change categorcial data column to numerical data.\n",
    "# To turn categorcial data column, get_dummies function is used.\n",
    "# Example: if there exists 1 feature column called gender with 2 values {male, female}, \n",
    "#         get_dummies function will converts to 2 columns: gender_male, gender_female (with values 0,1)\n",
    "\n",
    "# Note that 'Item_Identifier', 'Outlet_Identifier' are IDs; they have too many unique values \n",
    "# If we convert these two columns to numerical data, we will get too many resulting columns \n",
    "# Hence, we will drop these two columns\n",
    "\n",
    "df=orig_df.copy()\n",
    "temp_df=df.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)\n",
    "\n",
    "dummy_df=pd.get_dummies(temp_df)  \n",
    "dummy_df.columns    \n",
    "# dummy_df has 35 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vars = ['Item_Weight', 'Item_Visibility', 'Item_MRP',\n",
    "       'Outlet_Establishment_Year', 'Item_Fat_Content_LF',\n",
    "       'Item_Fat_Content_Low Fat', 'Item_Fat_Content_Regular',\n",
    "       'Item_Fat_Content_low fat', 'Item_Fat_Content_reg',\n",
    "       'Item_Type_Baking Goods', 'Item_Type_Breads', 'Item_Type_Breakfast',\n",
    "       'Item_Type_Canned', 'Item_Type_Dairy', 'Item_Type_Frozen Foods',\n",
    "       'Item_Type_Fruits and Vegetables', 'Item_Type_Hard Drinks',\n",
    "       'Item_Type_Health and Hygiene', 'Item_Type_Household', 'Item_Type_Meat',\n",
    "       'Item_Type_Others', 'Item_Type_Seafood', 'Item_Type_Snack Foods',\n",
    "       'Item_Type_Soft Drinks', 'Item_Type_Starchy Foods', 'Outlet_Size_High',\n",
    "       'Outlet_Size_Medium', 'Outlet_Size_Small',\n",
    "       'Outlet_Location_Type_Tier 1', 'Outlet_Location_Type_Tier 2',\n",
    "       'Outlet_Location_Type_Tier 3', 'Outlet_Type_Grocery Store',\n",
    "       'Outlet_Type_Supermarket Type1', 'Outlet_Type_Supermarket Type2',\n",
    "       'Outlet_Type_Supermarket Type3']\n",
    "len(input_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regresssion with original 35 independent variables\n",
    "\n",
    "df = dummy_df.dropna()\n",
    "X = df[input_vars]\n",
    "y = df.Item_Outlet_Sales\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use heatmap to visualize missing value (null) positions\n",
    "df=dummy_df.copy()\n",
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing value percent for each variable\n",
    "null_percent = df.isnull().sum()/len(df)*100\n",
    "null_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values with mean\n",
    "df['Item_Weight'].fillna(df['Item_Weight'].mean(), inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After replacing missing values, re-check %missing data\n",
    "df.isnull().sum()/len(df)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform regresssion with 35 original independent features\n",
    "##         where missing values in Item_Weight is replaced with its mean\n",
    "\n",
    "X = df[input_vars]\n",
    "y = df.Item_Outlet_Sales\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of replacing missing values, let's drop features that have %missing ratios > threshold\n",
    "# Let's set threshold = acceptable_missing_ratio = 15 (otherwise, Item_Weight will not be dropped if we set higher than this.)\n",
    "\n",
    "# Below is to get name of features that has %missing values <= acceptable_missing_ratio\n",
    "# Note that input_vars = a list of 35 original features\n",
    "# Here, updated_vars = a list of features with %missing values <= acceptable_missing_ratio\n",
    "\n",
    "df = dummy_df.copy()\n",
    "acceptable_missing_ratio = 15\n",
    "updated_vars = []\n",
    "for i in range(0,len(input_vars)):  \n",
    "    # if %missing value of this feature is less than or at least threshold, keep this feature\n",
    "    if null_percent[i] <= acceptable_missing_ratio :          \n",
    "        updated_vars.append(input_vars[i])\n",
    "updated_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(updated_vars) \n",
    "# Since one column (Item_weight) has %missing data > 15%, number of features decreases from 35 to 34."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform regresssion with 34 independent variables\n",
    "##         Item_Weight with %missing values above threshold is dropped\n",
    "\n",
    "X = df[updated_vars]\n",
    "y = df.Item_Outlet_Sales\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Variance Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute variance of each feature\n",
    "df = dummy_df.copy()\n",
    "df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After scanning variances from all features above, set minimum variance threshold = 10 \n",
    "# Note that minimum variance threshold can be set to any value\n",
    "\n",
    "# Below is to remove features with low variances\n",
    "# Note that input_vars = a list of 35 original features\n",
    "# Here, updated_vars = a list of features with variance >= minimum variance threshold\n",
    "\n",
    "min_var_threshold = 10\n",
    "updated_vars = [ ]\n",
    "for i in range(0,len(df[input_vars].var())):\n",
    "\n",
    "    if df[input_vars].var()[i] >= min_var_threshold:   \n",
    "        updated_vars.append(df[input_vars].columns[i])\n",
    "updated_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above, there are 3 features with variance >= minimum variance threshold\n",
    "# Create new data frame with these 3 features and 1 output\n",
    "\n",
    "df2 = df[updated_vars]\n",
    "df2['Item_Outlet_Sales']=df.Item_Outlet_Sales\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform regresssion with 3 features with variance >= minimum variance threshold\n",
    "\n",
    "df2 = df2.dropna()\n",
    "X = df2[updated_vars]\n",
    "y = df2['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Correlation Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation of numerical data\n",
    "df = dummy_df.copy()\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(df.corr(),\n",
    "            square=True,\n",
    "            linewidths=0.25,    \n",
    "            linecolor=(0,0,0),\n",
    "            cmap=sns.color_palette(\"coolwarm\"),\n",
    "            annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform regresssion with 35 original independent variables\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[input_vars]\n",
    "y = df['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at correlation matrix, \n",
    "# since Item_Fat_Content_Low Fat and Item_Fat_Content_Regular are highly correlated, \n",
    "# drop one of them\n",
    "\n",
    "df.drop('Item_Fat_Content_Low Fat', 1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that input_vars = a list of 35 original features\n",
    "# Here, updated_vars = a list of features without Item_Fat_Content_Low Fat\n",
    "\n",
    "updated_vars = list(input_vars)\n",
    "updated_vars.remove('Item_Fat_Content_Low Fat')\n",
    "len(updated_vars )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform regresssion with 34 original independent variables\n",
    "##         with dropping one of 2 features with high collinearity: 'Item_Fat_Content_Low Fat' & 'Item_Fat_Content_Regular'\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[updated_vars]\n",
    "y = df['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]\n",
    "\n",
    "# After dropping one feature, model performance is improved \n",
    "# Using all 35 original features -> [1191.4341205493079, 0.4132916031409223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try further\n",
    "# Since Item_MRP is highly correlated with Item_Outlet_Sales, let's drop this Item_MRP and see the performance\n",
    "\n",
    "df=dummy_df.copy()\n",
    "df=df.drop('Item_MRP', 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_vars = list(input_vars)\n",
    "updated_vars.remove('Item_MRP')\n",
    "len(updated_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform regresssion with 34 original independent variables\n",
    "##         with dropping feature with highest correlation with output (Item_MRP)\n",
    "\n",
    "df = df.dropna()\n",
    "X = df[updated_vars]\n",
    "y = df.Item_Outlet_Sales\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]\n",
    "\n",
    "# As you can see, when high-correlated feature with output is dropped.\n",
    "#                 the performance drops significantly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dummy_df.copy()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_regression receives input features and output\n",
    "#              and returns f-statistic and p-value for f-test \n",
    "# f-test is to used to test whether model fits data well or not \n",
    "# the more f-statistic (the lower p-value), the better\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "fstat, pval = f_regression(df[input_vars], df.Item_Outlet_Sales)\n",
    "fstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold to select subset of features based on f-test statistic\n",
    "\n",
    "# Note that input_vars = a list of 35 original features\n",
    "# Here, updated_vars = a list of features selected from f-test statistic from forward feature selection\n",
    "\n",
    "f_value_threshold = 10  # can adjust this value \n",
    "updated_vars = []\n",
    "for i in range(0,len(input_vars)-1):\n",
    "    if fstat[i] >= f_value_threshold:\n",
    "        updated_vars.append(df[input_vars].columns[i])\n",
    "updated_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform regresssion with 12 features selected from f-test statistic from forward feature selection\n",
    "\n",
    "X = df[updated_vars]\n",
    "y = df['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dummy_df.copy()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RFE receives fit model and number of features to select (which can be adjusted)\n",
    "# rfe receives input features and output \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "X = df[input_vars]\n",
    "y = df['Item_Outlet_Sales']\n",
    "n_features = 10  # define by yourself to select important features\n",
    "\n",
    "rfe = RFE(LinearRegression(), n_features)\n",
    "model = rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfe masks features that are selected as 1\n",
    "model.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain coefficients of features in regression model\n",
    "model.estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that input_vars = a list of 35 original features\n",
    "# Here, updated_vars = a list of features selected from recursive feature elimination\n",
    "\n",
    "updated_vars = []\n",
    "for i in range(0,len(input_vars)-1):\n",
    "    if model.ranking_[i] == 1:\n",
    "        updated_vars.append(df[input_vars].columns[i])\n",
    "updated_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform regresssion with features selected from recursive feature elimination\n",
    "\n",
    "X = df[updated_vars]\n",
    "y = df['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]\n",
    "\n",
    "# R2 using features selected from rfe is quite low\n",
    "# With this data, about 27 features need to be selected to get R2 > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't need to know which features get selected from rfe, \n",
    "#     you can use result model to predict immediately\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "X = df[input_vars]\n",
    "y = df['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "n_features = 10  # define by yourself to select important features\n",
    "\n",
    "rfe = RFE(LinearRegression(), n_features)\n",
    "model = rfe.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression & Lasso (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dummy_df.copy()\n",
    "df = df.dropna()\n",
    "X = df[input_vars]\n",
    "y = df['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regresssion with original 35 independent variables\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "## Perform regresssion with embedded lasso \n",
    "## set alpha (weight of penalty term) = 1\n",
    "\n",
    "lasso = Lasso(alpha=1)\n",
    "model = lasso.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can look at values of coefficients of regression model \n",
    "\n",
    "[model.coef_, model.intercept_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Filter features that has non-zero coefficients and sort coefficient from min to max\n",
    "##  Note that argsort returns indices of array after sort\n",
    "##  At the end, coef array = non-zero coeffients, sorted from min to max\n",
    "##              update_vars = names of features corresponding to coef array\n",
    "\n",
    "index = np.argsort(model.coef_)\n",
    "coef = [model.coef_[index[i]] for i in range(len(model.coef_))  if model.coef_[index[i]] != 0]\n",
    "update_vars = [input_vars[index[i]] for i in range(len(model.coef_))  if model.coef_[index[i]] != 0]\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find out how many features have left after performing lasso with alpha = 1\n",
    "\n",
    "len(update_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run another regression with lasso + alpha = 10\n",
    "\n",
    "lasso2 = Lasso(alpha=10)\n",
    "model2 = lasso2.fit(X_train, y_train)\n",
    "y_pred2 = model2.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model2.coef_, model2.intercept_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Filter features that has non-zero coefficients and sort coefficient from min to max\n",
    "\n",
    "index = np.argsort(model2.coef_)\n",
    "coef2 = [model2.coef_[index[i]] for i in range(len(model2.coef_))  if model2.coef_[index[i]] != 0]\n",
    "update_vars2 = [input_vars[index[i]] for i in range(len(model2.coef_))  if model2.coef_[index[i]] != 0]\n",
    "coef2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_vars2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find out how many features have left after performing lasso with alpha = 10\n",
    "\n",
    "len(update_vars2)\n",
    "\n",
    "## Since we use larger alpha, the penalty term has more weight.\n",
    "## More coefficients of regression model will be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot coefficients of linear regression model, regression with lasso + alpha=1,  regression with lasso + alpha=10\n",
    "## Note that alpha parameter in plot indicates transparency of marker (it has nothing to do with alpha in lasso)\n",
    "\n",
    "plt.plot(input_vars,lm.coef_,alpha=0.5,linestyle='none',marker='*',markersize=10,color='red',label=r'Linear Regression',zorder=7) # zorder for ordering the markers\n",
    "plt.plot(input_vars,model.coef_,alpha=0.5,linestyle='none',marker='o',markersize=10,color='blue',label=r'Lasso; $\\alpha = 1$') # alpha here is for transparency\n",
    "plt.plot(input_vars,model2.coef_,alpha=0.5,linestyle='none',marker='d',markersize=15,color='green',label=r'Lasso; $\\alpha = 10$') # alpha here is for transparency\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc='lower left')\n",
    "plt.xticks(input_vars, input_vars, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression & Ridge (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "## Perform regresssion with ridge \n",
    "## set alpha (weight of penalty term) = 1\n",
    "\n",
    "X = df[input_vars]\n",
    "y = df['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "model = ridge.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.coef_, model.intercept_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Filter features that has non-zero coefficients and sort coefficient from min to max\n",
    "##  Note that argsort returns indices of array after sort\n",
    "##  At the end, coef array = non-zero coeffients, sorted from min to max\n",
    "##              update_vars = names of features corresponding to coef array\n",
    "\n",
    "index = np.argsort(model.coef_)\n",
    "coef = [model.coef_[index[i]] for i in range(len(model.coef_))  if model.coef_[index[i]] != 0]\n",
    "update_vars = [input_vars[index[i]] for i in range(len(model.coef_))  if model.coef_[index[i]] != 0]\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find out how many features have left after performing ridge with alpha = 1\n",
    "\n",
    "len(update_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run another regression with ridge + alpha = 10\n",
    "\n",
    "ridge = Ridge(alpha=10)\n",
    "model2 = ridge.fit(X_train, y_train)\n",
    "y_pred = model2.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model2.coef_, model2.intercept_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(update_vars2)\n",
    "\n",
    "## Using ridge with alpha = 10 gives number of features = 34, like using alpha = 1\n",
    "## Hence, regression with ridge model does not help with feature selection much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot coefficients of linear regression model, regression with ridge + alpha=1,  regression with ridge + alpha=10\n",
    "## Note that alpha parameter in plot indicates transparency of marker (it has nothing to do with alpha in ridge)\n",
    "\n",
    "plt.plot(input_vars,lm.coef_,alpha=0.5,linestyle='none',marker='*',markersize=10,color='red',label=r'Linear Regression',zorder=7) # zorder for ordering the markers\n",
    "plt.plot(input_vars,model.coef_,alpha=0.5,linestyle='none',marker='o',markersize=10,color='blue',label=r'Ridge; $\\alpha = 1$') # alpha here is for transparency\n",
    "plt.plot(input_vars,model2.coef_,alpha=0.5,linestyle='none',marker='d',markersize=15,color='green',label=r'Ridge; $\\alpha = 10$') # alpha here is for transparency\n",
    "plt.xlabel('Coefficient Index',fontsize=16)\n",
    "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
    "plt.legend(fontsize=13,loc='lower left')\n",
    "plt.xticks(input_vars, input_vars, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dummy_df.copy()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestRegressor receives parameters for random forest such as depth of tree, number of trees\n",
    "\n",
    "# n_features = number of selected features\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(max_depth=10,n_estimators=100)\n",
    "model.fit(df[input_vars],df.Item_Outlet_Sales)\n",
    "\n",
    "n_features = 15\n",
    "\n",
    "# Plot feature importance\n",
    "features = df[input_vars].columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-n_features:]  # sort top n features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "# Choose features with large feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that input_vars = a list of 35 original features\n",
    "# Here, updated_vars = a list of 15 features selected from feature importance of random forest\n",
    "important_features = [features[i] for i in indices]\n",
    "important_features\n",
    "updated_vars = []\n",
    "for i in range(-1,-n_features-1,-1):\n",
    "    updated_vars.append(important_features[i])\n",
    "updated_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform regresssion with features selected from feature importance of random forest\n",
    "\n",
    "X = df[updated_vars]\n",
    "y = df['Item_Outlet_Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_pred = lm.predict(X_test)\n",
    "[np.sqrt(metrics.mean_squared_error(y_test,y_pred)),metrics.r2_score(y_test,y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
